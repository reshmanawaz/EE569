{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red89\green138\blue67;\red23\green23\blue23;\red202\green202\blue202;
\red70\green137\blue204;\red167\green197\blue152;\red194\green126\blue101;}
{\*\expandedcolortbl;;\cssrgb\c41569\c60000\c33333;\cssrgb\c11765\c11765\c11765;\cssrgb\c83137\c83137\c83137;
\cssrgb\c33725\c61176\c83922;\cssrgb\c70980\c80784\c65882;\cssrgb\c80784\c56863\c47059;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 # CIFAR-10 Dataset\cf4 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torch\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torch.nn \cf5 \strokec5 as\cf4 \strokec4  nn\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torch.nn.functional \cf5 \strokec5 as\cf4 \strokec4  F\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torch.optim \cf5 \strokec5 as\cf4 \strokec4  optim\cb1 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torchvision \cf5 \strokec5 as\cf4 \strokec4  tv\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  torchvision.transforms \cf5 \strokec5 as\cf4 \strokec4  transforms\cb1 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  matplotlib.pyplot \cf5 \strokec5 as\cf4 \strokec4  plt\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  numpy \cf5 \strokec5 as\cf4 \strokec4  np\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  math\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  os\cb1 \
\
\cf5 \cb3 \strokec5 from\cf4 \strokec4  torch.optim.lr_scheduler \cf5 \strokec5 import\cf4 \strokec4  StepLR\cb1 \
\cf5 \cb3 \strokec5 from\cf4 \strokec4  sklearn.metrics \cf5 \strokec5 import\cf4 \strokec4  confusion_matrix\cb1 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  seaborn \cf5 \strokec5 as\cf4 \strokec4  sns\cb1 \
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 train_batch_size = \cf6 \strokec6 64\cf4 \cb1 \strokec4 \
\cb3 test_batch_size = \cf6 \strokec6 1000\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 class\cf4 \strokec4  Net(nn.Module):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     \cf5 \strokec5 def\cf4 \strokec4  __init__(self):\cb1 \
\cb3         super(Net, \cf5 \strokec5 self\cf4 \strokec4 ).__init__()\cb1 \
\cb3         \cf5 \strokec5 self\cf4 \strokec4 .conv1 = nn.Conv2d(\cf6 \strokec6 3\cf4 \strokec4 , \cf6 \strokec6 6\cf4 \strokec4 , \cf6 \strokec6 5\cf4 \strokec4 )  \cf2 \strokec2 # Adjust input channels to 3 for CIFAR-10\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 self\cf4 \strokec4 .conv2 = nn.Conv2d(\cf6 \strokec6 6\cf4 \strokec4 , \cf6 \strokec6 16\cf4 \strokec4 , \cf6 \strokec6 5\cf4 \strokec4 )\cb1 \
\cb3         \cf5 \strokec5 self\cf4 \strokec4 .fc1 = nn.Linear(\cf6 \strokec6 16\cf4 \strokec4  * \cf6 \strokec6 5\cf4 \strokec4  * \cf6 \strokec6 5\cf4 \strokec4 , \cf6 \strokec6 120\cf4 \strokec4 )\cb1 \
\cb3         \cf5 \strokec5 self\cf4 \strokec4 .fc2 = nn.Linear(\cf6 \strokec6 120\cf4 \strokec4 , \cf6 \strokec6 84\cf4 \strokec4 )\cb1 \
\cb3         \cf5 \strokec5 self\cf4 \strokec4 .fc3 = nn.Linear(\cf6 \strokec6 84\cf4 \strokec4 , \cf6 \strokec6 10\cf4 \strokec4 )  \cf2 \strokec2 # Adjust output to 10 for CIFAR-10\cf4 \cb1 \strokec4 \
\
\cb3     \cf5 \strokec5 def\cf4 \strokec4  forward(self, x):\cb1 \
\cb3         x = F.max_pool2d(F.relu(\cf5 \strokec5 self\cf4 \strokec4 .conv1(x)), \cf6 \strokec6 2\cf4 \strokec4 )\cb1 \
\cb3         x = F.max_pool2d(F.relu(\cf5 \strokec5 self\cf4 \strokec4 .conv2(x)), \cf6 \strokec6 2\cf4 \strokec4 )\cb1 \
\cb3         x = x.view(-\cf6 \strokec6 1\cf4 \strokec4 , \cf5 \strokec5 self\cf4 \strokec4 .num_flat_features(x))\cb1 \
\cb3         x = F.relu(\cf5 \strokec5 self\cf4 \strokec4 .fc1(x))\cb1 \
\cb3         x = F.relu(\cf5 \strokec5 self\cf4 \strokec4 .fc2(x))\cb1 \
\cb3         x = \cf5 \strokec5 self\cf4 \strokec4 .fc3(x)\cb1 \
\cb3         \cf5 \strokec5 return\cf4 \strokec4  x\cb1 \
\
\cb3     \cf5 \strokec5 def\cf4 \strokec4  num_flat_features(self, x):\cb1 \
\cb3         size = x.size()[\cf6 \strokec6 1\cf4 \strokec4 :]\cb1 \
\cb3         num_features = \cf6 \strokec6 1\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 for\cf4 \strokec4  s \cf5 \strokec5 in\cf4 \strokec4  size:\cb1 \
\cb3             num_features *= s\cb1 \
\cb3         \cf5 \strokec5 return\cf4 \strokec4  num_features\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  load_data():\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     transform = transforms.Compose([\cb1 \
\cb3         transforms.Resize((\cf6 \strokec6 32\cf4 \strokec4 , \cf6 \strokec6 32\cf4 \strokec4 )),  \cf2 \strokec2 # Resize images to 32x32 for CIFAR-10\cf4 \cb1 \strokec4 \
\cb3         transforms.ToTensor(),\cb1 \
\cb3         transforms.Normalize((\cf6 \strokec6 0.5\cf4 \strokec4 , \cf6 \strokec6 0.5\cf4 \strokec4 , \cf6 \strokec6 0.5\cf4 \strokec4 ), (\cf6 \strokec6 0.5\cf4 \strokec4 , \cf6 \strokec6 0.5\cf4 \strokec4 , \cf6 \strokec6 0.5\cf4 \strokec4 ))  \cf2 \strokec2 # Adjust normalization for CIFAR-10\cf4 \cb1 \strokec4 \
\cb3     ])\cb1 \
\cb3     train_set = tv.datasets.CIFAR10(\cb1 \
\cb3         root=\cf7 \strokec7 './data'\cf4 \strokec4 ,\cb1 \
\cb3         train=\cf5 \strokec5 True\cf4 \strokec4 ,\cb1 \
\cb3         download=\cf5 \strokec5 True\cf4 \strokec4 ,\cb1 \
\cb3         transform=transform\cb1 \
\cb3     )\cb1 \
\cb3     train_loader = torch.utils.data.DataLoader(\cb1 \
\cb3         train_set,\cb1 \
\cb3         batch_size=train_batch_size,\cb1 \
\cb3         shuffle=\cf5 \strokec5 True\cf4 \strokec4 ,\cb1 \
\cb3         num_workers=\cf6 \strokec6 2\cf4 \cb1 \strokec4 \
\cb3     )\cb1 \
\cb3     test_set = tv.datasets.CIFAR10(\cb1 \
\cb3         root=\cf7 \strokec7 './data'\cf4 \strokec4 ,\cb1 \
\cb3         train=\cf5 \strokec5 False\cf4 \strokec4 ,\cb1 \
\cb3         download=\cf5 \strokec5 True\cf4 \strokec4 ,\cb1 \
\cb3         transform=transform\cb1 \
\cb3     )\cb1 \
\cb3     test_loader = torch.utils.data.DataLoader(\cb1 \
\cb3         test_set,\cb1 \
\cb3         batch_size=test_batch_size,\cb1 \
\cb3         shuffle=\cf5 \strokec5 False\cf4 \strokec4 ,\cb1 \
\cb3         num_workers=\cf6 \strokec6 2\cf4 \cb1 \strokec4 \
\cb3     )\cb1 \
\cb3     print(\cf7 \strokec7 "Data loaded successfully..."\cf4 \strokec4 )\cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  train_loader, test_loader\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  accuracy(model, data_loader):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     \cf5 \strokec5 with\cf4 \strokec4  torch.no_grad():\cb1 \
\cb3         correct = \cf6 \strokec6 0\cf4 \cb1 \strokec4 \
\cb3         total = \cf6 \strokec6 0\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 for\cf4 \strokec4  data \cf5 \strokec5 in\cf4 \strokec4  data_loader:\cb1 \
\cb3             images, labels = data\cb1 \
\cb3             outputs = model(images)\cb1 \
\cb3             _, predicted = torch.max(outputs.data, \cf6 \strokec6 1\cf4 \strokec4 )\cb1 \
\cb3             total += labels.size(\cf6 \strokec6 0\cf4 \strokec4 )\cb1 \
\cb3             correct += (predicted == labels).sum().item()\cb1 \
\
\cb3     \cf5 \strokec5 return\cf4 \strokec4  (\cf6 \strokec6 100\cf4 \strokec4  * correct / total)\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  train(train_loader, test_loader, model, criterion, optimizer, scheduler, max_epoch):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     train_acc = []\cb1 \
\cb3     test_acc = []\cb1 \
\cb3     \cf5 \strokec5 for\cf4 \strokec4  epoch \cf5 \strokec5 in\cf4 \strokec4  range(max_epoch):\cb1 \
\cb3         model.train()\cb1 \
\cb3         running_loss = \cf6 \strokec6 0\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 for\cf4 \strokec4  i, data \cf5 \strokec5 in\cf4 \strokec4  enumerate(train_loader, \cf6 \strokec6 0\cf4 \strokec4 ):\cb1 \
\cb3             inputs, labels = data\cb1 \
\cb3             optimizer.zero_grad()\cb1 \
\cb3             outputs = model(inputs)\cb1 \
\cb3             loss = criterion(outputs, labels)\cb1 \
\cb3             loss.backward()\cb1 \
\cb3             optimizer.step()\cb1 \
\
\cb3             running_loss += loss.item()\cb1 \
\cb3             \cf5 \strokec5 if\cf4 \strokec4  i % \cf6 \strokec6 200\cf4 \strokec4  == \cf6 \strokec6 199\cf4 \strokec4 :\cb1 \
\cb3                 print(\cf7 \strokec7 "[epoch %d, iter %5d] loss: %.3f"\cf4 \strokec4  % (epoch+\cf6 \strokec6 1\cf4 \strokec4 , i+\cf6 \strokec6 1\cf4 \strokec4 , running_loss / \cf6 \strokec6 200\cf4 \strokec4 ))\cb1 \
\cb3                 running_loss = \cf6 \strokec6 0.0\cf4 \cb1 \strokec4 \
\
\
\cb3         \cf2 \strokec2 # Update learning rate\cf4 \cb1 \strokec4 \
\cb3         scheduler.step()\cb1 \
\
\cb3         train_acc_epoch = accuracy(model, train_loader)\cb1 \
\cb3         test_acc_epoch = accuracy(model, test_loader)\cb1 \
\cb3         train_acc.append(train_acc_epoch)\cb1 \
\cb3         test_acc.append(test_acc_epoch)\cb1 \
\cb3         print(\cf7 \strokec7 "epoch %d: train_acc %.3f, test_acc %.3f"\cf4 \strokec4  % (epoch+\cf6 \strokec6 1\cf4 \strokec4 , train_acc_epoch, test_acc_epoch))\cb1 \
\
\cb3     \cf5 \strokec5 return\cf4 \strokec4  train_acc, test_acc\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  display(train_acc, test_acc):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     fig, ax = plt.subplots()\cb1 \
\cb3     ax.plot(range(\cf6 \strokec6 1\cf4 \strokec4 , len(train_acc) + \cf6 \strokec6 1\cf4 \strokec4 ), train_acc, color=\cf7 \strokec7 'r'\cf4 \strokec4 , label=\cf7 \strokec7 'train_acc'\cf4 \strokec4 )\cb1 \
\cb3     ax.plot(range(\cf6 \strokec6 1\cf4 \strokec4 , len(test_acc) + \cf6 \strokec6 1\cf4 \strokec4 ), test_acc, color=\cf7 \strokec7 'b'\cf4 \strokec4 , label=\cf7 \strokec7 'test_acc'\cf4 \strokec4 )\cb1 \
\cb3     ax.legend(loc=\cf7 \strokec7 'lower right'\cf4 \strokec4 )\cb1 \
\cb3     plt.show()\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  test(model, test_loader):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     model.eval()\cb1 \
\cb3     all_preds = []\cb1 \
\cb3     all_labels = []\cb1 \
\cb3     \cf5 \strokec5 with\cf4 \strokec4  torch.no_grad():\cb1 \
\cb3         \cf5 \strokec5 for\cf4 \strokec4  data \cf5 \strokec5 in\cf4 \strokec4  test_loader:\cb1 \
\cb3             images, labels = data\cb1 \
\cb3             outputs = model(images)\cb1 \
\cb3             _, predicted = torch.max(outputs, \cf6 \strokec6 1\cf4 \strokec4 )\cb1 \
\cb3             all_preds.extend(predicted.cpu().numpy())\cb1 \
\cb3             all_labels.extend(labels.cpu().numpy())\cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  all_preds, all_labels\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 def\cf4 \strokec4  plot_confusion_matrix(y_true, y_pred, classes):\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     cm = confusion_matrix(y_true, y_pred)\cb1 \
\cb3     cm = cm.astype(\cf7 \strokec7 'float'\cf4 \strokec4 ) / cm.sum(axis=\cf6 \strokec6 1\cf4 \strokec4 )[:, np.newaxis]  \cf2 \strokec2 # normalize\cf4 \cb1 \strokec4 \
\cb3     plt.figure(figsize=(\cf6 \strokec6 10\cf4 \strokec4 , \cf6 \strokec6 8\cf4 \strokec4 ))\cb1 \
\cb3     sns.heatmap(cm, annot=\cf5 \strokec5 True\cf4 \strokec4 , fmt=\cf7 \strokec7 ".2f"\cf4 \strokec4 , cmap=\cf7 \strokec7 "Blues"\cf4 \strokec4 , xticklabels=classes, yticklabels=classes)\cb1 \
\cb3     plt.xlabel(\cf7 \strokec7 'Predicted labels'\cf4 \strokec4 )\cb1 \
\cb3     plt.ylabel(\cf7 \strokec7 'True labels'\cf4 \strokec4 )\cb1 \
\cb3     plt.title(\cf7 \strokec7 'Normalized Confusion Matrix'\cf4 \strokec4 )\cb1 \
\cb3     plt.show()\cb1 \
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 if\cf4 \strokec4  __name__ == \cf7 \strokec7 '__main__'\cf4 \strokec4 :\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     \cf2 \strokec2 # Input CIFAR-10\cf4 \cb1 \strokec4 \
\cb3     train_loader, test_loader = load_data()\cb1 \
\cb3     \cf2 \strokec2 # New model\cf4 \cb1 \strokec4 \
\cb3     net = Net()\cb1 \
\
\cb3     \cf2 \strokec2 # Training\cf4 \cb1 \strokec4 \
\cb3     learning_rate = \cf6 \strokec6 0.0008\cf4 \cb1 \strokec4 \
\cb3     max_epoch = \cf6 \strokec6 30\cf4 \cb1 \strokec4 \
\cb3     criterion = nn.CrossEntropyLoss()\cb1 \
\cb3     \cf2 \strokec2 #optimizer = optimizer = optim.Adam(net.parameters(), lr=learning_rate)\cf4 \cb1 \strokec4 \
\cb3     optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(\cf6 \strokec6 0.9\cf4 \strokec4 , \cf6 \strokec6 0.999\cf4 \strokec4 ))  \cf2 \strokec2 # You can adjust the betas parameter for momentum\cf4 \cb1 \strokec4 \
\
\cb3   \cf2 \strokec2 #Learning rate scheduler\cf4 \cb1 \strokec4 \
\cb3     step_size = \cf6 \strokec6 15\cf4 \strokec4   \cf2 \strokec2 # Step size for learning rate decay\cf4 \cb1 \strokec4 \
\cb3     gamma = \cf6 \strokec6 0.1\cf4 \strokec4      \cf2 \strokec2 # Multiplicative factor for learning rate decay\cf4 \cb1 \strokec4 \
\cb3     scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\cb1 \
\
\cb3     train_acc, test_acc = train(train_loader, test_loader, net, criterion, optimizer, scheduler, max_epoch)\cb1 \
\
\cb3     \cf2 \strokec2 # display(train_acc, test_acc)\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 # Get predictions and labels\cf4 \cb1 \strokec4 \
\cb3     all_preds, all_labels = test(net, test_loader)\cb1 \
\
\cb3     \cf2 \strokec2 # Plot confusion matrix\cf4 \cb1 \strokec4 \
\cb3     classes = [\cf7 \strokec7 'airplane'\cf4 \strokec4 , \cf7 \strokec7 'automobile'\cf4 \strokec4 , \cf7 \strokec7 'bird'\cf4 \strokec4 , \cf7 \strokec7 'cat'\cf4 \strokec4 , \cf7 \strokec7 'deer'\cf4 \strokec4 , \cf7 \strokec7 'dog'\cf4 \strokec4 , \cf7 \strokec7 'frog'\cf4 \strokec4 , \cf7 \strokec7 'horse'\cf4 \strokec4 , \cf7 \strokec7 'ship'\cf4 \strokec4 , \cf7 \strokec7 'truck'\cf4 \strokec4 ]\cb1 \
\cb3     plot_confusion_matrix(all_labels, all_preds, classes)\cb1 \
}